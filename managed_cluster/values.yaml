vllm-stack:
  servingEngineSpec:
    enableEngine: true
    runtimeClassName: ""
    labels:
      environment: "production"
      release: "apertus"
      
    modelSpec:
      - name: "apertus-8b"
        repository: "ghcr.io/forpublicai/vllm-apertus"
        tag: "latest"
        modelURL: "swiss-ai/Apertus-8B-Instruct-2509"
        replicaCount: 2
        
        requestCPU: 18
        requestMemory: "95Gi"
        requestGPU: 1
        
        limitCPU: 20
        limitMemory: "105Gi"
        
          
        vllmConfig:
          maxModelLen: 8192
          gpuMemoryUtilization: 0.8
          dtype: "auto"
          
        # These will be set via command line to avoid storing secrets in git
        # hf_token: "your-hugging-face-token"
          
        nodeSelectorTerms:
          - matchExpressions:
            - key: "node.kubernetes.io/instance-type"
              operator: "In"
              values:
              - "gpua5000.medium"
              
        affinity:
          podAntiAffinity:
            preferredDuringSchedulingIgnoredDuringExecution:
            - weight: 100
              podAffinityTerm:
                labelSelector:
                  matchExpressions:
                  - key: model
                    operator: In
                    values:
                    - apertus-8b
                topologyKey: kubernetes.io/hostname

  routerSpec:
    enableRouter: true
    replicaCount: 1
    serviceType: LoadBalancer
    servicePort: 80
    
    # Router should look for serving engines with these labels
    labels:
      environment: "production"
      release: "apertus"